# This can be a domain that points to a LAN IP address if you
# are accessing the site from another machine's browser or you
# are installing Tator in a VM.
domain: &domain <Insert static IP or domain>
# These domains will work the same way as the primary domain.
#aliases:
#  - domain: alias.duckdns.org
#    # Key filename that will be used in secret for this domain.
#    tlsKeyFile: alias_key.key
#    # Cert filename that will be used in secret for this domain.
#    tlsCertFile: alias_cert.crt
#    # Key secret name that will be used in secret for this domain.
#    tlsKeySecretName: alias-tls-key
#    # Cert secret name that will be used in secret for this domain.
#    tlsCertSecretName: alias-tls-cert
# Requests for these domains will be redirected to the actual domain.
#redirects:
#  - domain: mysite.duckdns.org
#    # Key filename that will be used in secret for this domain.
#    tlsKeyFile: mysite_key.key
#    # Cert filename that will be used in secret for this domain.
#    tlsCertFile: mysite_cert.crt
#    # Key secret name that will be used in secret for this domain.
#    tlsKeySecretName: mysite-tls-key
#    # Cert secret name that will be used in secret for this domain.
#    tlsCertSecretName: mysite-tls-cert
# Enables debug mode for gunicorn. Do NOT enable this in production.
tatorDebug: false
# Enables javascript minification.
useMinJs: true
cors:
  # Enable this to allow cross origin resource sharing (CORS).
  enabled: true
  # Value of Access-Control-Allow-Origin header
  origin: "http://localhost:8080"
  # Value of Access-Control-Allow-Methods header
  methods: '*'
# Enable this to turn on "down for maintenance" page.
maintenance: false
# Secret key for django. Feel free to change this.
djangoSecretKey: "9q@$1)+x+zh-3csau(zqhheos2e+ncygac#ol2)1@x2w#kkaer"
postgresHost: "postgis-svc"
# Postgres username. Some make commands expect username django, but otherwise
# it can be changed.
postgresUsername: "django"
# Postgres password. Change this for production.
postgresPassword: "django123"
redisHost: "tator-redis-master"
elasticsearchHost: "elasticsearch-master"
liveBucket:
  config_file: "<name of config file in configs directory>"
  store_type: "<one of AWS, MINIO, GCP, or OCI>"
  name: "<your bucket name>"
# If you are using the docker registry container for your registry, you can
# leave these, otherwise change user/pass to the credentials for your registry.
dockerUsername: "test"
dockerPassword: "test"
dockerRegistry: "localhost:32000"
systemImageRepo: "localhost:32000"
#podGCStrategy: "OnWorkflowCompletion"
#slackToken: "<Your slack API token>" # Optional, for slack notifications
#slackChannel: "<Your slack channel ID>" # Optional, for slack notifications
# Enable this to require HTTPS. Be sure to set true for production deployments!
requireHttps: false
certCron:
  # Enable this to enable a cron job to automatically update certificates
  # periodically from LetsEncrypt. If this is not provided, the Secret objects
  # tls-cert and tls-key must be created manually.
  enabled: false
maintenanceCron:
  # Enable this to allow maintenance cron jobs to run, such as garbage collection
  # of deleted database objects and database backups.
  enabled: true
migrations:
  # Enable this if database migrations are allowed.
  enabled: true
# List of storage classes for use by workflows. One of these will be randomly
# passed as a workflow parameter to algorithm workflows, and randomly selected
# for transcode workflows.
workflowStorageClasses:
  - microk8s-hostpath
pv:
  hostPath:
    path: "/media/kubernetes_share"
# Can optionally supply seperate pv for static files:
staticPv:
  enabled: false
#   nfsServer: "192.168.1.220"
#   nfsMountOptions:
#     - nfsvers=4
#     - nolock
#   path: "/media/kubernetes_alt"
#
pvc:
  size: 10Ti
uploadBucket:
  enabled: false
  config_file: "<name of config file in configs directory>"
  store_type: "<one of AWS, MINIO, GCP, or OCI>"
  name: "<your bucket name>"
  # proxy: "<proxy domain, if required>"
backupBucket:
  enabled: false
  config_file: "<name of config file in configs directory>"
  store_type: "<one of AWS, MINIO, GCP, or OCI>"
  name: "<your bucket name>"
  # proxy: "<proxy domain, if required>"
hpa:
  nginxMinReplicas: 1
  nginxMaxReplicas: 1
  nginxCpuPercent: 50
  gunicornMinReplicas: 1
  gunicornMaxReplicas: 1
  gunicornCpuPercent: 50
# Transcode resource settings
transcoderMaxRamDiskSize: 2Gi
transcoderCpuLimit: 1000m
transcoderMemoryLimit: 2Gi
transcoderCodecNodeSelectors: false
# Gunicorn resource settings
gunicornCpuLimit: 4000m
gunicornCpuRequest: 1000m
gunicornMemoryLimit: 4Gi
gunicornMemoryRequest: 1Gi
# NGINX resource settings
nginxCpuLimit: 1000m
nginxCpuRequest: 250m
nginxMemoryLimit: 1Gi
nginxMemoryRequest: 250Mi
metallb:
  # Enable this to provide a load balancer implementation on bare metal.
  enabled: false
postgis:
  # Enable this if you want to use the postgis docker image.
  enabled: true
  persistence:
    size: 10Gi
  hostPath: "/media/kubernetes_share/postgis"
redis:
  # Enable this to install the redis helm chart.
  enabled: true
  architecture: standalone
  master:
    persistence:
      enabled: false
  nodeSelector:
    dbServer: "yes"
  usePassword: false
  auth:
    enabled: false
metrics-server:
  enabled: false
elasticsearch:
  # Enable this to install the elasticsearch helm chart.
  enabled: false
  hostPath: "/media/kubernetes_share/elasticsearch"
  persistence:
    enabled: true
  replicas: 1
  clusterHealthCheckParams: wait_for_status=yellow&timeout=1s
  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    resources:
      requests:
        storage: 30Gi
  nodeSelector: 
    dbServer: "yes"
filebeat:
  enabled: false
  image: docker.elastic.co/beats/filebeat-oss
  imageTag: 7.10.2
  filebeatConfig:
    filebeat.yml: |
      filebeat.inputs:
      - type: container
        paths:
        - /var/log/containers/*.log
        processors:
        - add_kubernetes_metadata:
            host: '${NODE_NAME}'
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
      output.elasticsearch:
        hosts: ['${ELASTICSEARCH_HOST:elasticsearch-master}:${ELASTICSEARCH_PORT:9200}']
      setup.ilm.enabled: false
kibana:
  enabled: false
  image: docker.elastic.co/kibana/kibana-oss
  imageTag: 7.10.2
  kibanaConfig:
    kibana.yml: |
      server:
        basePath: /logs
minio:
  enabled: true
  accessKey: "AKIAIOSFODNN7EXAMPLE"
  secretKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
  config_file: "example_minio_storage_config.json"
  name: tator
  store_type: "MINIO"
  persistence:
    size: "1Ti"
    existingClaim: "main-pv-claim"
    subPath: "objects"
  defaultBucket:
    enabled: true
    name: tator
kube-prometheus-stack:
  enabled: false
prometheus-adapter:
  enabled: false
remoteTranscodes:
  # Typically for dev processing is done on the same machine.
  enabled: false
  # Host/port are obtained via the following (run on the transcode cluster):
  #   echo $(kubectl config view --minify | grep server | cut -f 2- -d ":" | tr -d " ")
  host: "your.transcode.domain.org"
  port: "6443"
  # Token can be obtained via the following (run on the transcode cluster):
  #   SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ')
  #   TOKEN=$(kubectl describe secret $SECRET_NAME | grep -E '^token' | cut -f2 -d':' | tr -d " ")
  #   echo $TOKEN
  token: "Bearer <Your token here>"
  # Certificate can be obtained via the following (run on the transcode cluster):
  #   SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ')
  #   CERT=$(kubectl get secret $SECRET_NAME -o yaml | grep -E '^  ca.crt' | cut -f2 -d':' | tr -d " ")
  #   echo $CERT | base64 --decode
  cert: |
    -----BEGIN CERTIFICATE-----
    <Insert certificate here>
    -----END CERTIFICATE-----
cognito:
  enabled: false
  config: |
    aws-region: us-east-2
    pool-id: <POOL ID HERE>
    client-id: <ID HERE>
okta:
  enabled: false
  oauth2_key: "fill me in"
  oauth2_secret: "fill me in"
  oauth2_token_uri: "fill me in"
  oauth2_issuer: "fill me in"
  oauth2_auth_uri: "fill me in"
saml:
  enabled: false
  metadata_url: "fill me in"
email:
  enabled: false
  sender: "<fill in>"
  aws_region: "<fill in>"
  aws_access_key_id: "<fill in>"
  aws_secret_access_key: "<fill in>"
anonymousRegistration:
  # Enable to allow anyone to register to use the site.
  enabled: true
  # Enable to require email confirmation for anonymous registration. Automatic email 
  # must be configured.
  emailConfirmation: false
organizations:
  # Autocreate on new user registration.
  autocreate: true
  # Disable to allow POST for staff only.
  allowPost: true

