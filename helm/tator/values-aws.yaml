domain: &domain www.yourdomain.com
# These domains will work the same way as the primary domain.
#aliases:
#  - domain: alias.duckdns.org
#    # Key filename that will be used in secret for this domain.
#    tlsKeyFile: alias_key.key
#    # Cert filename that will be used in secret for this domain.
#    tlsCertFile: alias_cert.crt
#    # Key secret name that will be used in secret for this domain.
#    tlsKeySecretName: alias-tls-key
#    # Cert secret name that will be used in secret for this domain.
#    tlsCertSecretName: alias-tls-cert
# Requests for these domains will be redirected to the actual domain.
#redirects:
#  - domain: mysite.duckdns.org
#    # Key filename that will be used in secret for this domain.
#    tlsKeyFile: mysite_key.key
#    # Cert filename that will be used in secret for this domain.
#    tlsCertFile: mysite_cert.crt
#    # Key secret name that will be used in secret for this domain.
#    tlsKeySecretName: mysite-tls-key
#    # Cert secret name that will be used in secret for this domain.
#    tlsCertSecretName: mysite-tls-cert
cors:
  # Enable this to allow cross origin resource sharing (CORS).
  enabled: true
  # Value of Access-Control-Allow-Origin header
  origin: "http://localhost:8080"
  # Value of Access-Control-Allow-Methods header
  methods: '*'
# Enable this to turn on "down for maintenance" page.
maintenance: false
nginxReplicas: 5
gunicornReplicas: 20
djangoSecretKey: "<Your django secret key>"
postgresHost: "<Your RDS DB identifier>.clsla2zlnxez.<Your aws region>.rds.amazonaws.com"
postgresUsername: "<Your postgres username>"
postgresPassword: "<Your postgres password>"
redisHost: "<Your ElastiCache endpoint>"
elasticsearchHost: "https://<Your Amazon Elasticsearch Service VPC endpoint>"
objectStorageHost: "https://<Your S3 bucket endpoint>"
objectStorageBucketName: "<Your S3 bucket name>"
objectStorageRegionName: "<Your S3 bucket region>"
objectStorageAccessKey: "<Your S3 bucket access key>"
objectStorageSecretKey: "<Your S3 bucket secret key>"
dockerUsername: "<Your ECR username>"
dockerPassword: "<Your ECR password>"
dockerRegistry: "<Your aws account ID>.dkr.ecr.<Your aws region>.amazonaws.com"
# Enable this to require HTTPS. Be sure to set true for production deployments!
requireHttps: true
certCron:
  # Enable this to enable a cron job to automatically update certificates
  # periodically from LetsEncrypt. If this is not provided, the Secret objects
  # tls-cert and tls-key must be created manually. See scripts/cert.sh for an
  # example of how to do this.
  enabled: true
maintenanceCron:
  # Enable this to allow maintenance cron jobs to run, such as garbage collection
  # of deleted database objects and database backups.
  enabled: true
migrations:
  # Enable this if database migrations are allowed.
  enabled: true
# List of storage classes for use by workflows. One of these will be randomly
# passed as a workflow parameter to algorithm workflows, and randomly selected
# for transcode workflows.
workflowStorageClasses:
  - aws-efs
  - aws-efs-01
  - aws-efs-02
  - aws-efs-03
pv:
  nfsServer: "<Your efs filesystem ID>.efs.<Your aws region>.amazonaws.com"
  nfsMountOptions:
    - nfsvers=4.1
    - rsize=1048576
    - wsize=1048576
    - hard
    - timeo=600
    - retrans=2
    - noresvport
  path: "/"
uploadBucket:
  enabled: true
  host: "https://<Your S3 bucket endpoint>"
  name: "<Your S3 bucket name>"
  region: "<Your S3 bucket region>"
  accessKey: "<Your S3 bucket access key>"
  secretKey: "<Your S3 bucket secret key>"
backupBucket:
  enabled: false
  host: "https://<Your S3 bucket endpoint>"
  name: "<Your S3 bucket name>"
  region: "<Your S3 bucket region>"
  accessKey: "<Your S3 bucket access key>"
  secretKey: "<Your S3 bucket secret key>"
hpa:
  nginxMinReplicas: 2
  nginxMaxReplicas: 10
  nginxCpuPercent: 50
  gunicornMinReplicas: 4
  gunicornMaxReplicas: 10
  gunicornCpuPercent: 50
metallb:
  # A load balancer implementation is provided by AWS.
  enabled: false
postgis:
  enabled: false
redis:
  # Enable this to use the Redis helm chart installed as a dependency
  # instead of AWS Elasticache.
  enabled: false
elasticsearch:
  # Enable this to use the Elasticsearch helm chart installed as a
  # dependency instead of AWS Elasticsearch Service.
  enabled: false
filebeat:
  enabled: true
  image: docker.elastic.co/beats/filebeat-oss
  imageTag: 7.10.2
  extraEnvs:
  - name: ELASTICSEARCH_HOST
    value: "https://<Your Amazon Elasticsearch Service VPC endpoint>"
  filebeatConfig:
    filebeat.yml: |
      filebeat.inputs:
      - type: container
        paths:
        - /var/log/containers/*.log
        processors:
        - add_kubernetes_metadata:
            host: '${NODE_NAME}'
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
      output.elasticsearch:
        hosts: ['${ELASTICSEARCH_HOST:elasticsearch-master}:${ELASTICSEARCH_PORT:9200}']
      setup.ilm.enabled: false
kibana:
  enabled: true
  image: docker.elastic.co/kibana/kibana-oss
  imageTag: 7.10.2
  kibanaConfig:
    kibana.yml: |
      server:
        basePath: /logs
minio:
  enabled: false
kube-prometheus-stack:
  enabled: true
  prometheusOperator:
    resources:
      requests:
        cpu: 250m
        memory: 1Gi
  prometheus:
    server:
      extraArgs:
        web.external-url: /prometheus/
        web.route-prefix: "/"
    prometheusSpec:
      replicas: 1
      resources:
        requests:
          cpu: 500m
          memory: 2Gi
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: gp2
            resources:
              requests:
                storage: 1Gi
      additionalScrapeConfigs:
        - job_name: node_exporter
          scrape_interval: 10s
          metrics_path: "/metrics"
          static_configs:
            - targets: ["tator-prometheus-node-exporter:9100"]
        - job_name: statsd_exporter
          scrape_interval: 10s
          metrics_path: "/metrics"
          static_configs:
            - targets: ["tator-prometheus-statsd-exporter:9102"]
        - job_name: nginx_status
          scrape_interval: 10s
          metrics_path: "/metrics"
          static_configs:
            - targets: ["nginx-svc:9113"]
  grafana:
    grafana.ini:
      server:
        domain: *domain
        root_url: "%(protocol)s://%(domain)s/grafana/"
        serve_from_sub_path: true
      auth.anonymous:
        enabled: true
        org_role: Admin
prometheus-adapter:
  enabled: true
  prometheus:
    url: http://prometheus-operated
    port: 9090
  rules:
    default: false
    resource:
      cpu:
        containerQuery: sum(rate(container_cpu_usage_seconds_total{<<.LabelMatchers>>, container!=""}[3m])) by (<<.GroupBy>>)
        nodeQuery: sum(rate(container_cpu_usage_seconds_total{<<.LabelMatchers>>, id='/'}[3m])) by (<<.GroupBy>>)
        resources:
          overrides:
            node:
              resource: node
            namespace:
              resource: namespace
            pod:
              resource: pod
        containerLabel: container
      memory:
        containerQuery: sum(container_memory_working_set_bytes{<<.LabelMatchers>>, container!=""}) by (<<.GroupBy>>)
        nodeQuery: sum(container_memory_working_set_bytes{<<.LabelMatchers>>,id='/'}) by (<<.GroupBy>>)
        resources:
          overrides:
            node:
              resource: node
            namespace:
              resource: namespace
            pod:
              resource: pod
        containerLabel: container
      window: 3m
remoteTranscodes:
  # Enable this if you would like to do transcodes with a different
  # Kubernetes cluster, such as an on-premises cluster. Follow instructions
  # at doc/job-cluster.md to set up the cluster.
  enabled: true
  # Host/port are obtained via the following (run on the transcode cluster):
  #   echo $(kubectl config view --minify | grep server | cut -f 2- -d ":" | tr -d " ")
  host: "your.transcode.domain.org"
  port: "6443"
  # Token can be obtained via the following (run on the transcode cluster):
  #   SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ')
  #   TOKEN=$(kubectl describe secret $SECRET_NAME | grep -E '^token' | cut -f2 -d':' | tr -d " ")
  #   echo $TOKEN
  token: "Bearer <Your token here>"
  # Certificate can be obtained via the following (run on the transcode cluster):
  #   SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ')
  #   CERT=$(kubectl get secret $SECRET_NAME -o yaml | grep -E '^  ca.crt' | cut -f2 -d':' | tr -d " ")
  #   echo $CERT | base64 --decode
  cert: |
    -----BEGIN CERTIFICATE-----
    <Insert certificate here>
    -----END CERTIFICATE-----
okta:
  enabled: false
  oauth2_key: "fill me in"
  oauth2_secret: "fill me in"
  oauth2_token_uri: "fill me in"
  oauth2_issuer: "fill me in"
  oauth2_auth_uri: "fill me in"
saml:
  enabled: false
  metadata_url: "fill me in"
  sso_url: "fill me in"
mfa:
  enabled: false
email:
  enabled: false
  sender: "<fill in>"
  aws_region: "<fill in>"
  aws_access_key_id: "<fill in>"
  aws_secret_access_key: "<fill in>"
